---
title: Adding Nodes to the Cluster
description: How to add Nodes to your Teleport cluster
---

This guide explains how to add Teleport Nodes to your cluster.

## Prerequisites

(!docs/pages/includes/edition-prereqs-tabs.mdx!)

- A Linux server that you will use to host your Teleport Node

(!docs/pages/includes/tctl.mdx!)

## Step 1/3. Install Teleport on your Node

<ScopedBlock scope={["oss", "enterprise"]}>

On the host where you will run your Teleport Node, follow the instructions for
your environment to install Teleport.

(!docs/pages/includes/install-linux.mdx!)

</ScopedBlock>
<ScopedBlock scope={["cloud"]}>

(!docs/pages/includes/install-agent-cloud.mdx!)

</ScopedBlock>

## Step 2/3. Join your Node to the cluster

In this section, we will join your Node to the Teleport cluster by:

- Obtaining information stored on the Auth Service
- Starting Teleport on your Node with the information we obtained

### Obtain a CA pin

In a zero-trust environment, you must assume that an attacker can hijack the IP
address of the Auth Service.

To prevent this from happening, you need to supply every new Node with
information about the Auth Service. This technique is called **CA pinning**. It
works by asking the Auth Service to produce a CA pin, a hash value of the SPKI
header in a certificate. This way, an attacker cannot easily forge a matching
private key.

If a CA pin is not provided, the Teleport Node will join a cluster but it will
print a warning message.

<Notice type="warning">

The CA pin becomes invalid if a Teleport administrator performs the CA rotation
by executing [`tctl auth rotate`](../reference/cli.mdx#tctl-auth-rotate).
    
</Notice>

Retrieve the CA pin of the Auth Service <ScopedBlock scope="cloud"> by running the following
command on your local machine</ScopedBlock>:

```code
$ export CA_PIN=$(tctl status | awk '/CA pin/{print $3}')

# Cluster  staging.example.com
# User CA  never updated
# Host CA  never updated
# CA pin   (=presets.ca_pin=)
```

### Generate a token

Teleport only allows access to Nodes that have joined the cluster.

Once a Node joins, it receives a host certificate signed by the cluster's
Auth Service. To receive a host certificate upon joining a cluster, a new
Teleport host must present an **invite token**.

An invite token also defines which role a new host can assume within a cluster:
`auth`, `proxy`, `node`, `app`, `kube`, or `db`.

Administrators can generate tokens as they are needed. A token can be used
multiple times until its time to live (TTL) expires.

Use the `tctl` tool <ScopedBlock scope="cloud">on your local
machine</ScopedBlock> to generate a new token. In the following example, a new
token is created with a TTL of five minutes:

```code
# Generate a short-lived invite token for a new node:
$ export INVITE_TOKEN=$(tctl nodes add --ttl=5m --roles=node | grep "invite token:" | grep -Eo "[0-9a-z]{32}")
$ echo ${INVITE_TOKEN}
# (=presets.tokens.first=)

# You can also list all generated non-expired tokens:
$ tctl tokens ls
# Token                            Type            Expiry Time
# ------------------------         -----------     ---------------
# (=presets.tokens.first=)         Node            25 Sep 18 00:21 UTC

# ... or revoke an invite token before it's used:
$ tctl tokens rm (=presets.tokens.first=)
```

If you want to provide your own token, you can do so using the `--token` flag:

```code
$ tctl nodes add --ttl=5m --roles=node,proxy --token=secret-value
# The invite token: secret-value
```

<Details scope={["oss","enterprise"]} title="An insecure alternative: static tokens" scopeOnly={true} opened>
<Admonition type="warning">
Use short-lived tokens instead of long-lived static tokens.
Static tokens are easier to steal, guess, and leak.
</Admonition>

Static tokens are defined ahead of time by an administrator and stored in the
auth server's config file:

```yaml
# Config section in `/etc/teleport.yaml` file for the auth server
auth_service:
    enabled: true
    tokens:
    # This static token allows new hosts to join the cluster as "proxy" or "node"
    - "proxy,node:secret-token-value"
    # A token can also be stored in a file. In this example the token for adding
    # new auth servers are stored in /path/to/tokenfile
    - "auth:/path/to/tokenfile"
```
</Details>

### Start your Node with the invite token and CA pin

<Tabs>
<TabItem scope={["oss", "enterprise"]} scopeOnly={true} label="Self Hosted">

Execute one of the following commands on a new Node to add it to a cluster.
Supply the invite token and CA pin you retrieved earlier:

```code
$ sudo teleport start \
   --roles=node \
   --token=(=presets.tokens.first=) \
   --ca-pin=(=presets.ca_pin=) \
   --auth-server=10.12.0.6:3025
```
</TabItem>
<TabItem scope={["cloud"]} scopeOnly={true} label="Teleport Cloud">

Execute the following command on a new Node to add it to a cluster. Replace
`mytenant.teleport.sh` with the domain name of your Teleport Cloud tenant.
Supply the invite token and CA pin you retrieved earlier:

```code
$ sudo teleport start \
   --roles=node \
   --token=(=presets.tokens.first=) \
   --ca-pin=(=presets.ca_pin=) \
   --auth-server=https://mytenant.teleport.sh:443
```

</TabItem>
</Tabs>

As new Nodes come online, they start sending ping requests every few seconds to
the Auth Service. This allows users to explore cluster membership and size:

```code
$ tctl nodes ls

Node Name     Node ID                                  Address            Labels
---------     -------                                  -------            ------
turing        d52527f9-b260-41d0-bb5a-e23b0cfe0f8f     10.1.0.5:3022      distro:ubuntu
dijkstra      c9s93fd9-3333-91d3-9999-c9s93fd98f43     10.1.0.6:3022      distro:debian
```

{/* TODO: This lengthy Details box should be a subsection. Using the Details box
as a workaround until we have a way to control the visibility of subsections
using the scope switcher */}

<ScopedBlock scope={["oss", "enterprise"]}>

### Teleport Node Tunneling

Teleport Node Tunneling lets you add a remote Node to an existing Teleport Cluster through a tunnel.
This can be useful for IoT applications or for managing a couple of servers in a different network.

<Admonition type="note">
We recommend setting up a [Trusted Cluster](../admin/trustedclusters.mdx) if you
have workloads split across different networks or clouds.
</Admonition>

To connect a Node to your cluster via Node Tunneling, use `tctl` to create a
single-use token for a Node. Instead of supplying the IP of the Auth Service for
the `--auth-server` flag, you will use the URL of the Proxy Service.

In the example below, we've replaced the auth server IP with the Proxy's web
endpoint `teleport-proxy.example.com:3080`.

```code
$ tctl tokens add --type=node | grep -oP '(?<=token:\s).*' > token.file

# This will save the token to a file.
# Run this on the new node to join the cluster:
$ sudo teleport start --roles=node --token=/path/to/token.file --auth-server=teleport-proxy.example.com:3080
```

Using the ports in Teleport's default configuration, the Node needs to be able
to talk to ports `3080` and `3024` on the Proxy Service. Port `3080` is used to
initially fetch the credentials (SSH and TLS certificates) and for discovering
the reverse tunnel. Port `3024 `is used to establish a connection to the Auth
Service through the Proxy.

For those using ACME, port `443` is also required. 

To enable multiplexing so only one port is used, simply set the
`tunnel_listen_addr` to the same value as `web_listen_addr` within the
`proxy_service` section of your configuration file. Teleport will automatically
recognize that the Proxy Service is using the same port for both addresses and
enable multiplexing.

If your log setting is set to DEBUG, you will see multiplexing enabled in the
server log.

```txt
DEBU [PROC:1]    Setup Proxy: Reverse tunnel proxy and web proxy listen on the same port, multiplexing is on. service/service.go:1944
```

<Admonition
  type="tip"
  title="Load Balancers"
>

  The setup above also works even if the cluster uses multiple Proxy Service
  instances behind a load balancer (LB) or a DNS entry with multiple values. In
  this case, the Node establishes a tunnel to every proxy.
  
  This requires that an LB
  uses a round-robin or a similar balancing algorithm. Do not use sticky load balancing algorithms (a.k.a. "session affinity") with Teleport Proxy Service instances.
</Admonition>

</ScopedBlock>

## Step 3/3. Revoke an invitation

Tokens used for joining Nodes to a cluster can be revoked before they are used.

Run the following command to create a token for a new Proxy Service.

```code
$ tctl nodes add --ttl=5m --roles=proxy 
# The invite token: (=presets.tokens.first=).
# This token will expire in 5 minutes.
# 
# Run this on the new node to join the cluster:
# 
# > teleport start \
#    --roles=proxy \
#    --token=(=presets.tokens.first=) \
#    --ca-pin=(=presets.ca_pin=) \
#    --auth-server=123.123.123.123:3025
# 
# Please note:
# 
#   - This invitation token will expire in 5 minutes
#   - 123.123.123.123 must be reachable from the new node
```

Next, run the following command to see a list of outstanding tokens:

```
$ tctl tokens ls

# Token                                Role       Expiry Time (UTC)
# -----                                ----       -----------------
# (=presets.tokens.first=)     Proxy      never
# (=presets.tokens.second=)     Node       17 May 16 03:51 UTC
# (=presets.tokens.third=)     Signup     17 May 16 04:24 UTC
```

<Admonition type="tip" title="Signup tokens">

The output of `tctl tokens ls` includes tokens used for adding users alongside
tokens used for adding Nodes to your cluster.

</Admonition>

In this example, the first token has a `never` expiry date because it is a
static token configured via a config file.

The token with the `Node` role was generated to invite a new Node to this
cluster. And the third token was generated to invite a new user to sign up.

Tokens created via `tctl` can be deleted (revoked) via the `tctl tokens del`
command. Run the following command to delete a token:

```code
$ tctl tokens del (=presets.tokens.first=)
# Token (=presets.tokens.first=) has been deleted
```